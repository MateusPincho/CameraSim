{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Camera calibration Sim\n",
    "\n",
    "Objective: know the camera calibration info to reconstruct the camera matrix. \n",
    "\n",
    "The mapping between a world point $P$ and your projection $p$ in the image plane is described by: \n",
    "\n",
    "$$ \\tilde{p} = M_{int} \\cdot M_{ext} \\cdot \\tilde{P} $$\n",
    "\n",
    "Where: \n",
    "\n",
    "$$ M_{int} = \\begin{bmatrix}\n",
    "f_x&0 &O_x \\\\\n",
    "0& f_y&O_y \\\\\n",
    "0& 0&1\n",
    "\\end{bmatrix} $$\n",
    "\n",
    "and \n",
    "\n",
    "$$ M_{ext} = \\begin{bmatrix}\n",
    "r_{11}&r_{12}  &r_{13} & t_x \\\\ r_{21} &r_{22} &r_{23}&t_y \\\\ r_{31}&r_{32} &r_{33}  & t_z\\\\ 0&0&0&1\n",
    "\\end{bmatrix} $$\n",
    "\n",
    "$M_{int}$ represents the internal parameters of the camera. \n",
    "- $(f_x,f_y)$ are the focal distance in pixels, in the $x$ and $y$ directions\n",
    "- $(O_x,O_y)$ is the principal point \n",
    "\n",
    "$M_{ext}$ represents the position and orientation of the camera in the world coordinate frame \n",
    "\n",
    "---\n",
    "\n",
    "### This parameters must be recovers this parameters from the simulation. \n",
    "\n",
    "The internal parameters must be recover from the vision sensor properties. \n",
    "\n",
    "<p align = \"center\">\n",
    "    <img src = '../vision-sensor.jpeg'>\n",
    "</p>\n",
    "\n",
    "**Fist problem:** Cannot set the focal length.\n",
    "- Use the perspective angle to set the focal length.\n",
    "- $ \\alpha = 2\\cdot\\text{atan}(\\frac{d}{2f})$, where $\\alpha$ is the perspective angle, $d$ is the sensor size and $f$ is the focal length.\n",
    "- This parameters can be recovered from the Raspberry Pi Camera V2 Documentation. \n",
    "\n",
    "**Second problem:** Cannot know the pixel density.\n",
    "- The $K$ is definied using the focal lenght in pixels units, not in milimeters. \n",
    "- To convert, it's necessary to know the pixel density of the vision sensor. \n",
    "- Set to the pixel density of the V2 Cam? \n",
    "\n",
    "--- \n",
    "\n",
    "The camera extrinsic parameters are recovered from the simulation using `sim.getObjectMatrix()`. This function returns the position and orientation of an object in relation to another\n",
    "\n",
    "<p align = \"center\">\n",
    "    <img src = '../object-matrix.jpeg'>\n",
    "</p>\n",
    "\n",
    "Your arguments are: \n",
    "- objectHandle\n",
    "- relativeToObjectHandle: indicates relative to which reference frame the matrix is specified.\n",
    "    - Specify sim.handle_world to set the absolute transformation matrix, sim.handle_inverse to set the inverse of the absolute transformation matrix, sim.handle_parent to set the transformation matrix relative to the object's parent, or an object handle relative to whose reference frame the transformation matrix is specified. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import librarys\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.linalg import svd, rq\n",
    "from coppeliasim_zmqremoteapi_client import RemoteAPIClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CoppeliaSim Scene config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Connect and configure the simulation \n",
    "client = RemoteAPIClient()\n",
    "sim = client.getObject('sim')\n",
    "\n",
    "# When simulation is not running, ZMQ message handling could be a bit\n",
    "# slow, since the idle loop runs at 8 Hz by default. So let's make\n",
    "# sure that the idle loop runs at full speed for this program:\n",
    "defaultIdleFps = sim.getInt32Param(sim.intparam_idle_fps)   \n",
    "sim.setInt32Param(sim.intparam_idle_fps, 0)\n",
    "\n",
    "# Get the vision sensor handle\n",
    "visionSensorHandle = sim.getObject('/Vision_sensor')\n",
    "cubo = sim.getObject('/Cuboid[3]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recover camera parameters from scene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62.369949317476774\n",
      "[[-1.00000000e+00 -8.27292428e-13 -2.06823107e-13 -4.55348106e-01]\n",
      " [-2.06823107e-13 -1.03472786e-13  1.00000000e+00 -1.36104780e+00]\n",
      " [-8.27292428e-13  1.00000000e+00  1.03472786e-13  6.58429641e-01]]\n",
      "[[-1.00000000e+00 -8.27292428e-13 -2.06823107e-13]\n",
      " [-2.06823107e-13 -1.03472786e-13  1.00000000e+00]\n",
      " [-8.27292428e-13  1.00000000e+00  1.03472786e-13]] \n",
      " [[-0.45534811]\n",
      " [-1.3610478 ]\n",
      " [ 0.65842964]]\n",
      "The camera matrix is: \n",
      " [[-2.52320000e+03  6.40000000e+02 -4.55633481e-10 -7.27539370e+02]\n",
      " [-8.19052596e-10  3.60000000e+02  2.52016000e+03 -3.19302355e+03]\n",
      " [-8.27292428e-13  1.00000000e+00  1.03472786e-13  6.58429641e-01]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Vision sensor internal parameters\n",
    "# Run this before to adjust the perspective angle\n",
    "def get_perspective_angle(focal_length, sensor_size):\n",
    "    perspective_angle = 2*math.atan(sensor_size / (2*focal_length))\n",
    "    perspective_angle = perspective_angle*(180/math.pi)\n",
    "    print(perspective_angle)\n",
    "    return perspective_angle\n",
    "\n",
    "get_perspective_angle(3.04,3.68)\n",
    "\n",
    "focal_length = 3.04\n",
    "# Defined in vision sensor parameters\n",
    "image_size = (1280,720)\n",
    "\n",
    "# Pixel density defined by the sensor resolution\n",
    "pixel_density = (830,829)\n",
    "\n",
    "# Cria a matriz K\n",
    "k_matrix = np.array([[focal_length*pixel_density[0], 0, image_size[0]/2],\n",
    "                    [0, focal_length*pixel_density[1], image_size[1]/2],\n",
    "                    [0, 0, 1]], dtype=np.float64)\n",
    "\n",
    "\n",
    "\n",
    "# Get Vision_sensor transformation matrix\n",
    "transformation_matrix = sim.getObjectMatrix(visionSensorHandle, -1)\n",
    "\n",
    "transformation_matrix = np.array(transformation_matrix).reshape(3,4)\n",
    "print(transformation_matrix)\n",
    "\n",
    "rotation_matrix = transformation_matrix[:,:3]\n",
    "translation_vector = transformation_matrix[:,3:]\n",
    "\n",
    "print(rotation_matrix, \"\\n\", translation_vector )\n",
    "\n",
    "sim.stopSimulation()\n",
    "\n",
    "# Cria a matriz de câmera C\n",
    "camera_matrix = k_matrix @ transformation_matrix\n",
    "print(\"The camera matrix is: \\n\",camera_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the calibration images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isZhang = False\n",
    "\n",
    "# Start simulation in CoppeliaSim\n",
    "sim.startSimulation()\n",
    "\n",
    "# index of the calibration image\n",
    "n = 1\n",
    "\n",
    "# See the Vision sensor image\n",
    "while (t := sim.getSimulationTime()) < 2:\n",
    "    img, resX, resY = sim.getVisionSensorCharImage(visionSensorHandle)\n",
    "    img = np.frombuffer(img, dtype=np.uint8).reshape(resY, resX, 3)\n",
    "    # In CoppeliaSim images are left to right (x-axis), and bottom to top (y-axis)\n",
    "    # (consistent with the axes of vision sensors, pointing Z outwards, Y up)\n",
    "    # and color format is RGB triplets, whereas OpenCV uses BGR:\n",
    "    img = cv2.flip(cv2.cvtColor(img, cv2.COLOR_BGR2RGB), 0)\n",
    "    cv2.imshow('', img)\n",
    "    cv2.waitKey(1)\n",
    "\n",
    "    #p=sim.getObjectPosition(cubo,-1)\n",
    "\n",
    "    #print(p, transformation_matrix,'\\n','\\n')\n",
    "    #time.sleep(0.75)\n",
    "\n",
    "    cv2.imwrite(f'../calibration-images/image{n}.jpg',img)\n",
    "\n",
    "sim.stopSimulation()\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "sim.stopSimulation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract world points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 2. 0.]\n",
      " [1. 0. 0.]\n",
      " [2. 0. 0.]\n",
      " [3. 0. 0.]\n",
      " [0. 3. 1.]\n",
      " [0. 2. 1.]\n",
      " [0. 1. 1.]\n",
      " [1. 0. 1.]\n",
      " [2. 0. 1.]\n",
      " [3. 0. 1.]\n",
      " [0. 3. 2.]\n",
      " [0. 2. 2.]\n",
      " [0. 1. 2.]\n",
      " [1. 0. 2.]\n",
      " [2. 0. 2.]\n",
      " [3. 0. 2.]\n",
      " [0. 3. 3.]\n",
      " [0. 2. 3.]\n",
      " [0. 1. 3.]\n",
      " [1. 0. 3.]\n",
      " [2. 0. 3.]\n",
      " [3. 0. 3.]\n",
      " [1. 1. 3.]\n",
      " [2. 3. 3.]\n",
      " [2. 2. 3.]\n",
      " [2. 1. 3.]\n",
      " [3. 2. 3.]]\n"
     ]
    }
   ],
   "source": [
    "if isZhang: \n",
    "    patternSize = (7,7)\n",
    "    squareSize = (10)\n",
    "    imgSize = (1280,720)\n",
    "\n",
    "    def construct3DPoints(patternSize,squareSize):\n",
    "        X = np.zeros((patternSize[0]*patternSize[1],3), np.float32)\n",
    "        X[:,:2] = np.mgrid[0:patternSize[0],0:patternSize[1]].T.reshape(-1,2)\n",
    "        X = X * squareSize\n",
    "        return X\n",
    "\n",
    "    boardPoints = construct3DPoints(patternSize,squareSize)\n",
    "    worldPoints = []\n",
    "\n",
    "else: \n",
    "    #Pontos no cubo que estão visiveis \n",
    "    worldPoints = np.array([[0.,2.,0.],\n",
    "                            [1.,0.,0.],\n",
    "                            [2.,0.,0.],\n",
    "                            [3.,0.,0.],\n",
    "                            [0.,3.,1.],\n",
    "                            [0.,2.,1.],\n",
    "                            [0.,1.,1.],\n",
    "                            [1.,0.,1.],\n",
    "                            [2.,0.,1.],\n",
    "                            [3.,0.,1.],\n",
    "                            [0.,3.,2.],\n",
    "                            [0.,2.,2.],\n",
    "                            [0.,1.,2.],\n",
    "                            [1.,0.,2.],\n",
    "                            [2.,0.,2.],\n",
    "                            [3.,0.,2.],\n",
    "                            [0.,3.,3.],\n",
    "                            [0.,2.,3.],\n",
    "                            [0.,1.,3.],\n",
    "                            [1.,0.,3.],\n",
    "                            [2.,0.,3.],\n",
    "                            [3.,0.,3.],\n",
    "                            [1.,1.,3.],\n",
    "                            [2.,3.,3.],\n",
    "                            [2.,2.,3.],\n",
    "                            [2.,1.,3.],\n",
    "                            [3.,2.,3.]]) \n",
    "    print(worldPoints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract image points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if isZhang:\n",
    "    counter = 0\n",
    "    imagePoints = []\n",
    "    images = glob.glob('../calibration-images/*.jpg')\n",
    "\n",
    "    for fname in images:\n",
    "        img = cv2.imread(fname)\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        ret, corners = cv2.findChessboardCornersSB(gray, patternSize, None)\n",
    "        if ret == True:\n",
    "            print(\"Corners found in image\" + str(fname)) #- see if corners are found \n",
    "            imagePoints.append(corners)\n",
    "            worldPoints.append(boardPoints)\n",
    "            counter+=1\n",
    "\n",
    "    print(\"using \" + str(counter) + \" images\")\n",
    "\n",
    "else: \n",
    "    img = cv2.imread('image1.jpg') \n",
    "  \n",
    "    # convert image to gray scale image \n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) \n",
    "    \n",
    "    # detect corners with the goodFeaturesToTrack function. \n",
    "    corners = cv2.goodFeaturesToTrack(gray, 27, 0.01, 10) \n",
    "    \n",
    "    print(\"number of corners detected: \",corners.shape[0])\n",
    "    \n",
    "    # Set the needed parameters to find the refined corners\n",
    "    winSize = (5, 5)\n",
    "    zeroZone = (-1, -1)\n",
    "    criteria = (cv2.TERM_CRITERIA_EPS + cv2.TermCriteria_COUNT, 40, 0.001)\n",
    "    # Calculate the refined corner locations\n",
    "    corners = cv2.cornerSubPix(gray, corners, winSize, zeroZone, criteria)\n",
    "\n",
    "    print(corners)\n",
    "\n",
    "    # we iterate through each corner,  \n",
    "    # making a circle at each point that we think is a corner. \n",
    "    corners = np.intp(corners) \n",
    "    for i in corners: \n",
    "        x, y = i.ravel()\n",
    "        cv2.circle(img, (x, y), 3, 255, -1) \n",
    "    \n",
    "    plt.imshow(img), plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For my implementation\n",
    "\n",
    "image_corners = np.array([[589.8389, 503.97058],\n",
    "                          [777.3517,  521.30194],\n",
    "                          [834.59064, 490.49197],\n",
    "                          [886.38837, 462.90256],\n",
    "                          [530.44305, 407.4186],\n",
    "                          [582.48773, 424.63785],\n",
    "                          [640.3677,  443.95663],\n",
    "                          [772.36536, 436.53787],\n",
    "                          [831.4153,  409.56305],\n",
    "                          [883.82324, 385.4333],\n",
    "                          [521.0537,  327.31122],\n",
    "                          [573.5912, 340.64044],\n",
    "                          [632.4177,  355.56662],\n",
    "                          [768.48425, 346.7792],\n",
    "                          [829.6393,  323.54968],\n",
    "                          [883.33,    303.22266],\n",
    "                          [510.4737,  245.18546],\n",
    "                          [564.3152,  252.68439],\n",
    "                          [625.03827, 261.8383 ],\n",
    "                          [762.4857,  252.42708],\n",
    "                          [826.42706, 233.55133],\n",
    "                          [882.011,   217.62466],\n",
    "                          [694.6928,  243.68637],\n",
    "                          [640.3723,  214.56644],\n",
    "                          [695.0399,  220.20595],\n",
    "                          [759.52905, 226.50145],\n",
    "                          [746.915,   206.28847]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the camera matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 20 of 20 images\n",
      "RMS re-projection error: 0.33705101242723445\n",
      "Camera Matrix:\n",
      " [[1.82969190e+04 0.00000000e+00 8.01309852e+02]\n",
      " [0.00000000e+00 1.83150351e+04 3.45041635e+02]\n",
      " [0.00000000e+00 0.00000000e+00 1.00000000e+00]]\n",
      "Distortion Parameters:\n",
      " [[ 4.33119347e+02  1.57864668e+03  8.46151169e-03 -5.17443479e-03\n",
      "   1.89869002e+01  4.37607075e+02 -1.59918539e+03  9.29244408e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "if isZhang: \n",
    "    criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 30, 0.001)\n",
    "    flagsCalib = cv2.CALIB_RATIONAL_MODEL\n",
    "    ret, cameraMatrix, k, rvecs, tvecs = cv2.calibrateCamera(worldPoints, imagePoints, imgSize, None, None,flags=flagsCalib)\n",
    "\n",
    "    print(\"Using \"+str(counter)+\" of \"+str(len(images))+\" images\")\n",
    "    print(\"RMS re-projection error:\", ret)\n",
    "    print(\"Camera Matrix:\\n\", cameraMatrix)\n",
    "    print(\"Distortion Parameters:\\n\", k)\n",
    "    \n",
    "else: \n",
    "\n",
    "    def calibrate_camera(world_points, image_points):\n",
    "        \"\"\"\n",
    "        Função para encontrar a matriz de câmera C usando o algoritmo DLT.\n",
    "        \n",
    "        Args: \n",
    "            world_points: Array de pontos 3D na cena.\n",
    "            image_points: Array de pontos 2D projetados na imagem.\n",
    "\n",
    "        Return: \n",
    "            C_matrix: numpy array contendo a matriz de câmera estimada. \n",
    "        \"\"\"\n",
    "\n",
    "        if len(world_points) != len(image_points): \n",
    "            raise ValueError(\"Must be the same number of World points and Image points\")\n",
    "        \n",
    "        if len(world_points) < 6 | len(image_points) < 6 :\n",
    "            raise ValueError(\"Must have a minimum of 6 points to compute the camera matrix\")\n",
    "\n",
    "\n",
    "        Q = []\n",
    "        for i in range(len(world_points)):\n",
    "            X, Y, Z = world_points[i]\n",
    "            u,v  = image_points[i]\n",
    "            Q.append([X, Y, Z, 1, 0, 0, 0, 0, -u*X, -u*Y, -u*Z, -u])\n",
    "            Q.append([0, 0, 0, 0, X, Y, Z, 1, -v*X, -v*Y, -v*Z, -v])\n",
    "\n",
    "        Q = np.array(Q)\n",
    "        #print(Q)\n",
    "\n",
    "        # Resolva o sistema de equações usando SVD\n",
    "        _, _, V = svd(Q)\n",
    "        c = V[-1, :12]\n",
    "        #print(c)\n",
    "\n",
    "        # Reconstrua a matriz de projeção da câmera\n",
    "        C_matrix = c.reshape(3, 4)\n",
    "        print(C_matrix)\n",
    "\n",
    "        return C_matrix\n",
    "    \n",
    "    camera_matrix = calibrate_camera(worldPoints,imagePoints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[   207.81465166,   -930.25728747]],\n",
       "\n",
       "       [[ -4937.10970747,  -4849.45292485]],\n",
       "\n",
       "       [[ -8769.25796515,  -4849.45292486]],\n",
       "\n",
       "       [[-12601.40622282,  -4849.45292487]],\n",
       "\n",
       "       [[   325.94876678,    111.28721708]],\n",
       "\n",
       "       [[   207.81465166,     17.73093893]],\n",
       "\n",
       "       [[   -52.78449452,   -188.65047919]],\n",
       "\n",
       "       [[ -4937.10970747,  -1021.92171329]],\n",
       "\n",
       "       [[ -8769.25796514,  -1021.92171329]],\n",
       "\n",
       "       [[-12601.40622282,  -1021.92171329]],\n",
       "\n",
       "       [[   325.94876678,    800.15108699]],\n",
       "\n",
       "       [[   207.81465166,    965.71916533]],\n",
       "\n",
       "       [[   -52.78449452,   1330.95574245]],\n",
       "\n",
       "       [[ -4937.10970747,   2805.60949828]],\n",
       "\n",
       "       [[ -8769.25796514,   2805.60949828]],\n",
       "\n",
       "       [[-12601.40622282,   2805.60949828]],\n",
       "\n",
       "       [[   325.94876678,   1489.0149569 ]],\n",
       "\n",
       "       [[   207.81465166,   1913.70739173]],\n",
       "\n",
       "       [[   -52.78449452,   2850.56196409]],\n",
       "\n",
       "       [[ -4937.10970747,   6633.14070984]],\n",
       "\n",
       "       [[ -8769.25796514,   6633.14070984]],\n",
       "\n",
       "       [[-12601.40622282,   6633.14070985]],\n",
       "\n",
       "       [[ -1574.22377554,   2850.56196409]],\n",
       "\n",
       "       [[ -1053.44088828,   1489.0149569 ]],\n",
       "\n",
       "       [[ -1690.44886562,   1913.70739173]],\n",
       "\n",
       "       [[ -3095.66305655,   2850.56196409]],\n",
       "\n",
       "       [[ -2639.58062426,   1913.70739173]]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Plots and stuff like that\n",
    "\n",
    "def extract_camera_parameters(projection_matrix):\n",
    "    # Decompor a matriz de projeção em RQ (decomposição de R e Q)\n",
    "    K, RT = rq(projection_matrix, mode='full')\n",
    "    \n",
    "    # Certificar-se de que a diagonal da matriz intrínseca K seja positiva\n",
    "    if K[0, 0] < 0:\n",
    "        K = -K\n",
    "        RT = -RT\n",
    "\n",
    "    # Extrair a matriz de rotação R\n",
    "    R = RT[:3, :3]\n",
    "\n",
    "    # Extrair o vetor de translação t\n",
    "    t = RT[:3, 3]\n",
    "\n",
    "    return K[:, 1:], R, t\n",
    "\n",
    "#Criar minha própria função para projetar os pontos\n",
    "#real_images_points = cv2.projectPoints(worldPoints, rotation_matrix, translation_vector, k_matrix, None)[0]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
